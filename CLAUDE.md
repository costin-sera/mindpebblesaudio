# MindPebbles — Voice Journaling with AI Audio Reflection  
**Hackathon Product Specification**  
_Last updated: 2025-12-10_

MindPebbles is a **lightweight, audio-first journaling web app** designed for rapid introspection.  
Users record short voice entries (“pebbles”), which the system transforms into:

- **Transcription**  
- **Emotional analysis**  
- **Psychological markers**  
- **Key topics**  
- **AI-generated spoken reflections** (via ElevenLabs)

Everything is displayed in a **timeline** of beautifully simple insight cards.

MindPebbles is optimized for **fast hackathon development** using a **frontend-only architecture**, direct API calls, local storage persistence, and minimal dependencies.

---

# 1. Product Overview

MindPebbles helps users drop “mental pebbles” into their day — small, easy voice logs that create ripples of insight.  

**Core Goals:**
- Capture voice quickly  
- Generate emotional + thematic understanding  
- Provide gentle, spoken reflections  
- Visualize entries in a timeline  
- Keep the prototype extremely simple to implement  

---

# 2. Architecture

MindPebbles is built as a **pure frontend web app**.  
No backend is required for the hackathon prototype.

### Why frontend-only?
- Fastest implementation  
- Zero server setup  
- ElevenLabs APIs accept browser requests with API keys  
- LocalStorage is enough for journaling timelines  
- Perfect for localhost demos  

**Tradeoff:**  
API keys will be visible in DevTools — acceptable for hackathons.

---

# 3. Technology Stack

### Frontend
- **React (Vite)** for speed and simplicity
- **MediaRecorder API** for capturing microphone audio
- **CSS modules or Tailwind** for quick styling
- **LocalStorage** for saving entries

### AI + Audio Services
- **ElevenLabs STT (Speech-to-Text)** — transcribe voice notes  
- **OpenAI / LLM** — analyze message + extract emotions/topics/psychological markers  
- **ElevenLabs TTS (Text-to-Speech)** — generate spoken reflection feedback  

---

# 4. User Experience Flow

## 4.1 Home Screen

Displays:

- Big round **Record** button  
- Dropdown for selecting **Voice Style** (ElevenLabs voice)  
- Timeline of past “pebbles” on the right  

## 4.2 Recording Flow

1. User presses **Record**  
2. Browser captures audio using `MediaRecorder`  
3. User presses **Stop**  
4. App calls:
   - ElevenLabs STT → transcript  
   - LLM → insight JSON  
   - ElevenLabs TTS → audio feedback  
5. MindPebbles creates a new **Journal Entry Card** and stores it in `localStorage`.

## 4.3 Reviewing Insights

Each entry shows:

- Emotions (chips or bars)  
- Psychological markers  
- Topics  
- Transcript (collapsible)  
- Summary  
- Original audio  
- AI-generated spoken reflection  

## 4.4 Timeline

Right-side panel listing all entries, newest first.  
Each card contains:

- Timestamp  
- Emotion color or icon  
- Short preview of topics  
- “Play feedback” mini-button  

Clicking a card loads it into the left detail pane.

---

# 5. Data Model

```json
{
  "id": "uuid",
  "createdAt": "2025-12-10T21:34:00Z",
  "transcript": "User's spoken words...",
  "summary": "Short summary of the journal entry...",
  "emotions": [
    { "name": "stress", "score": 0.72 },
    { "name": "hope", "score": 0.41 }
  ],
  "topics": ["work pressure", "self-doubt"],
  "psychMarkers": [
    { "name": "rumination", "level": "high", "description": "..." }
  ],
  "feedbackText": "Spoken reflection generated by AI...",
  "feedbackAudioUrl": "blob:.../ai-feedback.mp3",
  "originalAudioUrl": "blob:.../user-recording.webm",
  "voiceId": "elevenlabs_voice_id"
}
```

Stored in `localStorage` under key:  
```
mindpebbles_entries
```

---

# 6. Core Functional Specification

## 6.1 Recording Component  
`Recorder.js`

- Start/stop button  
- Uses `navigator.mediaDevices.getUserMedia`  
- Stores audio in Blob format  
- Emits finished audio blob to parent `App.jsx`

## 6.2 STT (Speech-to-Text)  
Called directly via fetch:

```
POST https://api.elevenlabs.io/v1/speech-to-text
Headers: xi-api-key
Body: multipart/form-data (file)
Returns: { text: "transcript" }
```

## 6.3 Insight Analysis (LLM)

Prompt returns structured JSON with:
- summary  
- emotions  
- topics  
- psych_markers  
- feedback_text  

Must be parsed and validated client-side.

## 6.4 TTS (Text-to-Speech)

```
POST https://api.elevenlabs.io/v1/text-to-speech/{voice_id}
Headers: xi-api-key
Body: JSON: { text, model_id }
Returns: audio/mpeg
```

Converted into Blob → ObjectURL.

## 6.5 Timeline Component  
`Timeline.js`

- Displays all stored entries  
- Clicking loads the entry into detail view  

---

# 7. Insight Card Specification

Each **MindPebble Insight Card** contains:

### 1. Header
- Timestamp  
- Selected voice (icon or name)

### 2. Emotional Profile
- List of `{emotion, score}`  
- Visual style:
  - horizontal bars or color-coded pills  
  - example: Stress ●●●●○ (0.8)

### 3. Topics  
- Chips, e.g.  
  - `work stress`  
  - `identity`  

### 4. Psychological Markers  
Examples:
- rumination (high)  
- self-criticism (medium)  
- avoidance (low)

### 5. Transcript  
Collapsible section with full transcription.

### 6. Summary  
Two-line summary generated by AI.

### 7. Audio Players  
- Original user audio  
- AI feedback audio (ElevenLabs)

---

# 8. LocalStorage Behaviour

### Saving  
Every new entry adds to the array:
```
mindpebbles_entries = JSON.stringify([...entries, newEntry])
```

### Loading  
On app load:
```
const entries = JSON.parse(localStorage.getItem("mindpebbles_entries") || "[]")
```

### Updating  
Timeline renders automatically.

---

# 9. Styling

### Theme: **Calm, pebble-like, soft blues and sand tones**
- Rounded buttons  
- Soft shadows  
- Waveform-inspired graphics  
- Minimalist typography (Inter / Lato)

### Layout:
- Left: Active Entry / Recording  
- Right: Timeline (fixed width ~350px)

---

# 10. Development Setup

### Install

```
npm create vite@latest mindpebbles --template react
cd mindpebbles
npm install
npm run dev
```

### Environment Variables  
Stored in `.env` (visible to FE in hackathon):

```
ELEVENLABS_API_KEY=...
OPENAI_API_KEY=...
```

Access via `import.meta.env`.

---

# 11. Limitations (Acceptable for Hackathon)

- API keys exposed  
- No server-side audio storage  
- No user accounts  
- Limited privacy (localhost only)  
- No pagination or advanced search  

---

# 12. Future Enhancements (Post-hackathon)

- Secure backend to hide keys  
- Advanced emotion timeline graph  
- Word clouds from topics  
- "Compare me over time" insights  
- Daily prompts  
- Pebble streaks  
- Export as audio “memory reel”

---

# 13. Summary

**MindPebbles** is a lightweight personal voice journal where emotional insights surface automatically and AI speaks back with comforting, human-like reflections.

Optimized for:
- **Speed of development**  
- **Audio-first interaction**  
- **Immediate emotional value**  
- **Hackathon demonstration appeal**



This is for a 3 hour hackaton
These are the judging criteria we’ll use to evaluate projects:

Using ElevenLabs is mandatory:

Try to use some of these techonlogies

AI Tinkerers

n8n

Bolt

BlackBox

CodeRabbit

Clerk

Stripe

Working Prototype	The submission must run end‑to‑end and demonstrate the agent’s core functionality.	1) Code does not run at all.
2) Major crashes or missing core features.
3) Partial functionality, major bugs remain.
4) Mostly stable with minor bugs.
5) Fully functional, polished, and ready for demo.
Technical Complexity & Integration	Depth of AI/LLM usage, tool‑use APIs, multimodal handling, and seamless integration of browser, voice, cloud, or tool agents.	1) Simple static mock‑up.
2) Basic API call without real‑time interaction.
3) Moderate use of one modality (e.g., only voice).
4) Sophisticated multi‑modal integration (e.g., browser + voice).
5) Advanced orchestration across several agents with robust error handling and real‑world data pipelines.
Innovation & Creativity	Novelty of the agent concept, clever problem framing, and original solution design.	1) Replicates an existing demo.
2) Minor tweak of known idea.
3) Useful but not surprising.
4) Creative twist or new application of known tech.
5) Groundbreaking agent that opens a new use‑case or paradigm.
Real‑World Impact	Potential to solve a tangible problem, user value, scalability, and deployment readiness.	1) Purely academic or toy example.
2) Interesting but limited user relevance.
3) Solves a niche need.
4) Addresses a clear market or social need.
5) Demonstrates life‑changing or large‑scale impact potential (e.g., accessibility, productivity gains).
Theme Alignment	How well the project embodies the hackathon’s focus on conversational agents.	1) Barely related to agents (e.g., generic UI)
2) Superficial mention of agents without functional demo.
3) Agent that uses one partner technology but ignores all other.
4) Clear multi‑agent implementation that showcases at least two partner technologies.
5) Exemplifies the theme perfectly, integrating conversational agents into a cohesive autonomous assistant.
